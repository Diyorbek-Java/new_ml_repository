{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Import required libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Machine Learning imports\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\n\n# Metrics\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    confusion_matrix, classification_report, roc_curve, auc,\n    roc_auc_score, precision_recall_curve\n)\n\n# Visualization\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# Model persistence\nimport joblib\nimport os\n\n# Set style\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (12, 6)\n\nprint(\"✅ All libraries imported successfully!\")"
  },
  {
   "cell_type": "markdown",
   "source": "## 1. Data Loading and Initial Exploration\n\nLoad the FEMA disaster declarations dataset and perform initial inspection.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load the dataset\ndata_path = '../data/disaster_declarations.csv'\ndf = pd.read_csv(data_path)\n\nprint(f\"Dataset Shape: {df.shape}\")\nprint(f\"\\nRows: {df.shape[0]:,}\")\nprint(f\"Columns: {df.shape[1]}\")\nprint(\"\\n\" + \"=\"*50)\nprint(\"Dataset loaded successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Display first few rows\nprint(\"First 5 rows of the dataset:\")\ndf.head()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Display dataset information\nprint(\"Dataset Information:\")\nprint(\"=\"*70)\ndf.info()\nprint(\"\\n\" + \"=\"*70)\nprint(\"\\nData Types Distribution:\")\nprint(df.dtypes.value_counts())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Check for missing values\nprint(\"Missing Values Analysis:\")\nprint(\"=\"*70)\nmissing = df.isnull().sum()\nmissing_pct = (df.isnull().sum() / len(df)) * 100\nmissing_df = pd.DataFrame({\n    'Column': missing.index,\n    'Missing_Count': missing.values,\n    'Percentage': missing_pct.values\n})\nmissing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n\nif len(missing_df) > 0:\n    print(missing_df.to_string(index=False))\nelse:\n    print(\"✅ No missing values found in the dataset!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Statistical summary\nprint(\"Statistical Summary of Numerical Features:\")\nprint(\"=\"*70)\ndf.describe().T",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}